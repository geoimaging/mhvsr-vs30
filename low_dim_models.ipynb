{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71723193-0d87-4bdc-ab6a-f21df8f621e4",
   "metadata": {},
   "source": [
    "# Estimation of Vs30 Using the Low-Dimensional Models\n",
    "\n",
    "## License Information\n",
    "\n",
    "This file is part of _mHVSR-Vs30_, a collection of data-driven models\n",
    "for predicting Vs30 from mHVSR.\n",
    "\n",
    "    Copyright (C) 2025 Sharma Wagle, Rodriguez-Marek, Vantassel (joseph.p.vantassel@gmail.com)\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https: //www.gnu.org/licenses/>.\n",
    "    \n",
    "## About _mHVSR-Vs30_\n",
    "\n",
    "`mHVSR-Vs30` is a collection of data-driven models to predict the\n",
    "time-averaged shear wave velocity in the upper 30 m (Vs30), from \n",
    "microtremor horizontal-to-vertical spectral ratio (mHVSR). The developed\n",
    "models developed include both low-dimensional (`low_dim_models.ipynb`) and\n",
    "high-dimensional (`high_dim_models.ipynb`). The details of the model's\n",
    "development and performance are presented in the reference below.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use `mHVSR-Vs30` in your research or consulting, we ask you please cite the following:\n",
    "\n",
    "> Sharma Wagle, K., Vantassel, J.P., and Rodriguez-Marek, A. (2025). \"A Set of Data-Driven Models to Predict VS30 from the\n",
    "> Horizontal-to-Vertical Spectral Ratio of Microtremors\". Bulletin of the Seismological Society of America. [In-Review]\n",
    "\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook illustrates how `mHVSR-Vs30` estimates Vs30 from mHVSR measurements.\n",
    "\n",
    "The processing has been done following the SESAME (2004) guidelines.\n",
    "If you use this notebook, please also cite SESAME (2004) to recognize their original work.\n",
    "\n",
    "> SESAME (2004). Guidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations\n",
    "> Measurements, Processing, and Interpretation. European Commission - Research General Directorate, 62,\n",
    "> European Commission - Research General Directorate.\n",
    "\n",
    "To use this notebook, you need at least one clear resonance peak in the mHVSR curve. For more information regarding the clear resonance peak, see the SESAME (2004) guidelines. If your mHVSR peak fails the clarity, it is __not recommended__ to proceed further with the prediction.\n",
    "\n",
    "In addition to a measurement of ambient noise (from which an mHVSR measurement can be made), you need two topographic features: station elevation and the average elevation around the station across a 1500 m diameter circle centered on the station. For the topographic features use the 1 arc-second Digital Elevation Model (DEM) for consistency with the models development. If you do not have topographic features for your data, the notebook `high_dim_models.ipynb` can be used with only the microtremor recording (no topographic features required).\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Follow the instructions in the software's [README.md](https://github.com/geoimaging/mhvsr-vs30?tab=readme-ov-file#getting-started) to get started by downloading the software and installing the dependencies.\n",
    "2. To run the default example, open this notebook in JupyterLab and select `Kernel > Restart Kernel and Run All Cells`.\n",
    "3. To try the other examples, uncomment the corresponding example number in the cell labeled __Data Input and Recording Check__ below before selecting `Kernel > Restart Kernel and Run All Cells`.\n",
    "4. Once you are comfortable running the examples provided, try supplying your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad2cba5-6d18-4a3e-82ab-8ee2824ff419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvsrpy\n",
    "from hvsrpy import sesame\n",
    "import obspy\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew\n",
    "import joblib\n",
    "\n",
    "plt.style.use(hvsrpy.postprocessing.HVSRPY_MPL_STYLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96887ae-0243-4bdf-a7ce-7cdc94b84145",
   "metadata": {},
   "source": [
    "### Data Input and Recording Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92abd95a-02cc-48f9-ab01-a8ecc3199d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed duration is 60.00 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Data Input\n",
    "\n",
    "# example 1\n",
    "fname = \"./data/AR.STN01.A1.2019.006.mseed\"\n",
    "elevation_in_m =  2246\n",
    "elevation_1500m_avg_in_m =  2237.903656\n",
    "\n",
    "# # example 2\n",
    "# fname = \"./data/CI.FUR-Rec288-Sen450.mseed\"\n",
    "# elevation_in_m =  -44 #in meters.\n",
    "# elevation_1500m_avg_in_m =  -43.80603662 # in meters.\n",
    "\n",
    "# # example 3\n",
    "# fname = \"./data/UT.STN09_20130320_020000.mseed\"\n",
    "# elevation_in_m =  35 #in meters.\n",
    "# elevation_1500m_avg_in_m =  35.33800078 # in meters.\n",
    "\n",
    "stream = obspy.read(fname)\n",
    "\n",
    "# Check trace count\n",
    "if len(stream) != 3:\n",
    "    print(\"Recording should have exactly three traces.\")\n",
    "else:\n",
    "    # Find latest start time and earliest end time\n",
    "    start_times = [tr.stats.starttime for tr in stream]\n",
    "    end_times = [tr.stats.endtime for tr in stream]\n",
    "\n",
    "    common_start = max(start_times)\n",
    "    common_end = min(end_times)\n",
    "\n",
    "    # Check duration\n",
    "    common_duration = common_end - common_start\n",
    "    if common_duration <= 0:\n",
    "        print(\"No overlapping time window between traces.\")\n",
    "    else:\n",
    "        # Trim all traces to common time window\n",
    "        stream.trim(starttime=common_start, endtime=common_end)\n",
    "\n",
    "        # Save to MiniSEED file\n",
    "        fpath = pathlib.Path(fname)\n",
    "        fname_updated = f\"{fname[:len(fpath.suffix)]}_updated{fpath.suffix}\"\n",
    "        stream.write(fname_updated, format=\"MSEED\")\n",
    "\n",
    "        # Check if duration exceeds 30 minutes\n",
    "        if common_duration >= 1800:  # 1800 seconds = 30 minutes\n",
    "            print(f\"Trimmed duration is {common_duration/60 :.2f} minutes.\")\n",
    "        else:\n",
    "            print(\"Trimmed duration is less than 30 minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198aa980-0eca-4344-ad09-abf5a552b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Summary\n",
      "------------------------------------------------------------\n",
      "hvsrpy_version                           : 2.0.0\n",
      "orient_to_degrees_from_north             : 0.0\n",
      "filter_corner_frequencies_in_hz          : [None, None]\n",
      "window_length_in_seconds                 : 102.85714285714286\n",
      "detrend                                  : linear\n",
      "ignore_dissimilar_time_step_warning      : False\n",
      "preprocessing_method                     : hvsr\n",
      "Processing Summary\n",
      "------------------------------------------------------------\n",
      "hvsrpy_version                           : 2.0.0\n",
      "window_type_and_width                    : ('tukey', 0.2)\n",
      "smoothing                                :\n",
      "     operator                            : konno_and_ohmachi\n",
      "     bandwidth                           : 40\n",
      "     center_frequencies_in_hz            : [0.14776046176014435 ... 6371930790751, 50.0]\n",
      "fft_settings                             : None\n",
      "handle_dissimilar_time_steps_by          : frequency_domain_resampling\n",
      "processing_method                        : traditional\n",
      "method_to_combine_horizontals            : geometric_mean\n"
     ]
    }
   ],
   "source": [
    "# HVSR Preprocessing Settings\n",
    "preprocessing_settings = hvsrpy.settings.HvsrPreProcessingSettings()\n",
    "preprocessing_settings.detrend = \"linear\"\n",
    "significant_cycles = 15  # require 15 significant cycles.\n",
    "time_windows = 35  # require 35 time windows.\n",
    "duration_in_seconds = common_duration  # window length (s)\n",
    "windowlength_in_seconds = duration_in_seconds / time_windows\n",
    "preprocessing_settings.window_length_in_seconds = windowlength_in_seconds\n",
    "\n",
    "print(\"Preprocessing Summary\")\n",
    "print(\"-\"*60)\n",
    "preprocessing_settings.psummary()\n",
    "\n",
    "# HVSR Processing Settings\n",
    "processing_settings = hvsrpy.settings.HvsrTraditionalProcessingSettings()\n",
    "processing_settings.window_type_and_width = (\"tukey\", 0.2)\n",
    "processing_settings.smoothing=dict(operator=\"konno_and_ohmachi\",\n",
    "                                   bandwidth=40,\n",
    "                                   center_frequencies_in_hz=np.geomspace(0.05, 50, 256))\n",
    "processing_settings.method_to_combine_horizontals = \"geometric_mean\"\n",
    "processing_settings.handle_dissimilar_time_steps_by = \"frequency_domain_resampling\"\n",
    "\n",
    "desired_frequency_vector_in_hz = np.geomspace(0.05, 50, 256)\n",
    "minimum_frequency = significant_cycles / windowlength_in_seconds\n",
    "fids = desired_frequency_vector_in_hz > minimum_frequency\n",
    "frequency_resampling_in_hz = desired_frequency_vector_in_hz[fids]\n",
    "processing_settings.smoothing[\"center_frequencies_in_hz\"] = frequency_resampling_in_hz\n",
    "\n",
    "print(\"Processing Summary\")\n",
    "print(\"-\"*60)\n",
    "processing_settings.psummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f38bf2-82f2-465d-b57f-6c07b192c31f",
   "metadata": {},
   "source": [
    "### HVSR Processing and Manual Window Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c6bd22-127f-4bf8-900c-7861ba547a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute HVSR\n",
    "srecords = hvsrpy.read([fname_updated])\n",
    "srecords = hvsrpy.preprocess(srecords, preprocessing_settings)\n",
    "hvsr = hvsrpy.process(srecords, processing_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d2a816-2082-40a0-b600-cc8bf27df9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HvsrTraditional object\n",
    "mhvsr = hvsrpy.HvsrTraditional(frequency=hvsr.frequency, amplitude=hvsr.amplitude)\n",
    "\n",
    "# Perform manual window rejection\n",
    "hvsrpy.window_rejection.manual_window_rejection(\n",
    "    mhvsr, y_limit=15, plot_frequency_std=False, fig=None, ax=None  #Change y_limit as required in the plot.\n",
    ")\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f607493d-7375-46d6-8988-9b5a6cf99795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SESAME (2004) Clarity and Reliability Criteria:\n",
      "-----------------------------------------------\n",
      "\u001b[1mAssessing SESAME (2004) reliability criteria ... \u001b[0m\n",
      "  Criteria i): \u001b[32mPass\u001b[0m\n",
      "  Criteria ii): \u001b[32mPass\u001b[0m\n",
      "  Criteria iii): \u001b[32mPass\u001b[0m\n",
      "  The chosen peak \u001b[32mPASSES\u001b[0m the peak reliability criteria, with 3 of 3.\n",
      "\u001b[1mAssessing SESAME (2004) clarity criteria ... \u001b[0m\n",
      "  Criteria i): \u001b[32mPass\u001b[0m\n",
      "  Criteria ii): \u001b[32mPass\u001b[0m\n",
      "  Criteria iii): \u001b[32mPass\u001b[0m\n",
      "  Criteria iv): \u001b[32mPass\u001b[0m\n",
      "  Criteria v): \u001b[32mPass\u001b[0m\n",
      "  Criteria vi): \u001b[32mPass\u001b[0m\n",
      "  The chosen peak \u001b[32mPASSES\u001b[0m the peak clarity criteria, with 6 of 6.\n"
     ]
    }
   ],
   "source": [
    "search_range_in_hz = (None, None)\n",
    "verbose = 1\n",
    "\n",
    "print(\"\\nSESAME (2004) Clarity and Reliability Criteria:\")\n",
    "print(\"-\"*47)\n",
    "hvsrpy.sesame.reliability(\n",
    "    windowlength=preprocessing_settings.window_length_in_seconds,\n",
    "    passing_window_count=np.sum(mhvsr.valid_window_boolean_mask),\n",
    "    frequency=mhvsr.frequency,\n",
    "    mean_curve=mhvsr.mean_curve(distribution=\"lognormal\"),\n",
    "    std_curve=mhvsr.std_curve(distribution=\"lognormal\"),\n",
    "    search_range_in_hz=search_range_in_hz,\n",
    "    verbose=verbose,\n",
    ")\n",
    "hvsrpy.sesame.clarity(\n",
    "    frequency=mhvsr.frequency,\n",
    "    mean_curve=mhvsr.mean_curve(distribution=\"lognormal\"),\n",
    "    std_curve=mhvsr.std_curve(distribution=\"lognormal\"),\n",
    "    fn_std=mhvsr.std_fn_frequency(distribution=\"normal\"),\n",
    "    search_range_in_hz=search_range_in_hz,\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "fig, axs = hvsrpy.plot_single_panel_hvsr_curves(mhvsr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6d274-f36e-4004-89b5-5fbf7382853f",
   "metadata": {},
   "source": [
    "### Parameters for low dim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cac0256-9667-4028-af28-53ac6d0ee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_mean = mhvsr.mean_fn_frequency(distribution=\"lognormal\")\n",
    "an_mean = mhvsr.mean_fn_amplitude(distribution=\"lognormal\")\n",
    "TPI = elevation_in_m - elevation_1500m_avg_in_m\n",
    "\n",
    "# Function to compute skewness ignoring leading NaNs and filtering to 0.2â€“50 Hz\n",
    "def compute_skew_ignore_leading_nans(freqs, amps):\n",
    "    # Filter by frequency range\n",
    "    mask = (freqs > 0.199) & (freqs < 50)\n",
    "    filtered_amps = amps[mask]\n",
    "\n",
    "    # Ignore leading NaNs\n",
    "    first_valid_index = np.argmax(~np.isnan(filtered_amps))\n",
    "    trimmed = filtered_amps[first_valid_index:]\n",
    "    valid = trimmed[~np.isnan(trimmed)]\n",
    "\n",
    "    return skew(valid, bias=False) if len(valid) >= 3 else np.nan\n",
    "\n",
    "\n",
    "mhvsr_mean_curve = mhvsr.mean_curve(distribution=\"lognormal\")  #Taking the lognormal mean curve among the accepted windows.\n",
    "skewness = compute_skew_ignore_leading_nans(mhvsr.frequency, mhvsr_mean_curve)\n",
    "\n",
    "# Assemble into DataFrame\n",
    "X = pd.DataFrame([{\n",
    "    \"fn_mean\": fn_mean,\n",
    "    \"an_mean\": an_mean,\n",
    "    \"TPI\": TPI,\n",
    "    \"Skewness\": skewness,\n",
    "    \"elevation\": elevation_in_m\n",
    "}])\n",
    "\n",
    "# Elevation Binning\n",
    "elevation_bins = [-500, 0, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "elevation_labels = [\n",
    "    'Elevation_Bin_[-500.0, 0.0)',\n",
    "    'Elevation_Bin_[0.0, 500.0)',\n",
    "    'Elevation_Bin_[500.0, 1000.0)',\n",
    "    'Elevation_Bin_[1000.0, 1500.0)',\n",
    "    'Elevation_Bin_[1500.0, 2000.0)',\n",
    "    'Elevation_Bin_[2000.0, 2500.0)',\n",
    "    'Elevation_Bin_[2500.0, 3000.0)'\n",
    "]\n",
    "\n",
    "# Bin the elevation into categories\n",
    "X = X.copy()\n",
    "X['Elevation_Bin'] = pd.cut(X['elevation'], bins=elevation_bins, labels=elevation_labels, right=False)\n",
    "\n",
    "# One-hot encode the elevation bin\n",
    "elevation_dummies = pd.get_dummies(X['Elevation_Bin']).astype(int)\n",
    "\n",
    "# Concatenate back to original DataFrame\n",
    "X = pd.concat([X, elevation_dummies], axis=1)\n",
    "\n",
    "# Optional: Drop 'Elevation_Bin' column if you only need one-hot encoded version\n",
    "X = X.drop(columns=['elevation','Elevation_Bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73cbbe86-e334-4d8f-b6ba-5a7df4b314b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fn_mean</th>\n",
       "      <th>an_mean</th>\n",
       "      <th>TPI</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Elevation_Bin_[-500.0, 0.0)</th>\n",
       "      <th>Elevation_Bin_[0.0, 500.0)</th>\n",
       "      <th>Elevation_Bin_[500.0, 1000.0)</th>\n",
       "      <th>Elevation_Bin_[1000.0, 1500.0)</th>\n",
       "      <th>Elevation_Bin_[1500.0, 2000.0)</th>\n",
       "      <th>Elevation_Bin_[2000.0, 2500.0)</th>\n",
       "      <th>Elevation_Bin_[2500.0, 3000.0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.460113</td>\n",
       "      <td>11.509159</td>\n",
       "      <td>8.096344</td>\n",
       "      <td>2.209668</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fn_mean    an_mean       TPI  Skewness  Elevation_Bin_[-500.0, 0.0)  \\\n",
       "0  0.460113  11.509159  8.096344  2.209668                            0   \n",
       "\n",
       "   Elevation_Bin_[0.0, 500.0)  Elevation_Bin_[500.0, 1000.0)  \\\n",
       "0                           0                              0   \n",
       "\n",
       "   Elevation_Bin_[1000.0, 1500.0)  Elevation_Bin_[1500.0, 2000.0)  \\\n",
       "0                               0                               0   \n",
       "\n",
       "   Elevation_Bin_[2000.0, 2500.0)  Elevation_Bin_[2500.0, 3000.0)  \n",
       "0                               1                               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427265a-a8c3-493c-8336-38fa0c8ae877",
   "metadata": {},
   "source": [
    "### Load low-dimensional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a69292a4-f4f3-410d-a6ae-8aebd1223325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpvantassel/miniconda3/lib/python3.12/pickle.py:1760: UserWarning: [15:29:08] WARNING: /workspace/src/collective/../data/../common/error_msg.h:82: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  setstate(state)\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "linear_model = joblib.load(\"./models/linear_model.joblib\")\n",
    "decisiontree_model = joblib.load(\"./models/decision_tree_model.joblib\")\n",
    "randomforest_model = joblib.load(\"./models/random_forest_model.joblib\")\n",
    "xgboost_model = joblib.load(\"./models/xgboost_model.joblib\")\n",
    "\n",
    "# Feature Scaling for linear model\n",
    "X_linear = X.copy().to_numpy()\n",
    "\n",
    "boxcox_scaler = joblib.load(\"./models/linear_f0hvsr_scaler.pkl\") # fn_mean: Box-Cox Transform\n",
    "X_linear[:, 0] = boxcox_scaler.transform(X_linear[:, 0].reshape(-1, 1)).flatten()\n",
    "\n",
    "X_linear[:, 1] = np.log(X_linear[:, 1]) #an_mean: log transformation\n",
    "\n",
    "std_scaler_tpi = joblib.load(\"./models/linear_topo_scaler.pkl\") #TPI: Standard transformation\n",
    "X_linear[:, 2] = std_scaler_tpi.transform(X_linear[:, 2].reshape(-1, 1)).flatten()\n",
    "\n",
    "std_scaler_skew = joblib.load(\"./models/linear_skewness_scaler.pkl\") #Skewness: standard transformation\n",
    "X_linear[:, 3] = std_scaler_tpi.transform(X_linear[:, 3].reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf8244-2133-4e86-8e6c-90eb704c72b4",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7772a59a-80cd-48d7-bf64-56f0387b7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs30_pred_linear = linear_model.predict(X_linear)\n",
    "Vs30_pred_decisiontree = decisiontree_model.predict(X.to_numpy())\n",
    "Vs30_pred_randomforest = randomforest_model.predict(X.to_numpy())\n",
    "Vs30_pred_xgboost = xgboost_model.predict(X.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138ead1b-77cf-496b-b798-051a6e3e809f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vs30 (m/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Vs30 (m/s)\n",
       "0         Linear         113\n",
       "1  Decision Tree         102\n",
       "2  Random Forest          81\n",
       "3        XGBoost         120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack predictions and model labels\n",
    "models = [\"Linear\", \"Decision Tree\", \"Random Forest\", \"XGBoost\"]\n",
    "predictions = [\n",
    "    np.exp(Vs30_pred_linear),\n",
    "    np.exp(Vs30_pred_decisiontree),\n",
    "    np.exp(Vs30_pred_randomforest),\n",
    "    np.exp(Vs30_pred_xgboost)\n",
    "]\n",
    "\n",
    "# Create dataframe\n",
    "# Create dataframe with 0 decimal precision\n",
    "vs30_modelwise_df = pd.DataFrame({\n",
    "    \"Model\": np.repeat(models, [len(p) for p in predictions]),\n",
    "    \"Vs30 (m/s)\": np.round(np.concatenate(predictions), 0).astype(int)\n",
    "})\n",
    "\n",
    "#Show Results\n",
    "vs30_modelwise_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
