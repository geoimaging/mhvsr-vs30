{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fb0c14-1b87-4180-ab73-b07ad49dde1a",
   "metadata": {},
   "source": [
    "# Estimation of Vs30 Using the High-Dimensional Models\n",
    "\n",
    "## License Information\n",
    "\n",
    "This file is part of _mHVSR-Vs30_, a collection of data-driven models\n",
    "for predicting Vs30 from mHVSR.\n",
    "\n",
    "    Copyright (C) 2025 Sharma Wagle, Rodriguez-Marek, Vantassel (joseph.p.vantassel@gmail.com)\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https: //www.gnu.org/licenses/>.\n",
    "    \n",
    "## About _mHVSR-Vs30_\n",
    "\n",
    "`mHVSR-Vs30` is a collection of data-driven models to predict the\n",
    "time-averaged shear wave velocity in the upper 30 m (Vs30), from \n",
    "microtremor horizontal-to-vertical spectral ratio (mHVSR). The developed\n",
    "models developed include both low-dimensional (`low_dim_models.ipynb`) and\n",
    "high-dimensional (`high_dim_models.ipynb`). The details of the model's\n",
    "development and performance are presented in the reference below.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use `mHVSR-Vs30` in your research or consulting, we ask you please cite the following:\n",
    "\n",
    "> Sharma Wagle, K., Vantassel, J.P., and Rodriguez-Marek, A. (2025). \"A Set of Data-Driven Models to Predict VS30 from the\n",
    "> Horizontal-to-Vertical Spectral Ratio of Microtremors\". Bulletin of the Seismological Society of America. [In-Review]\n",
    "\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This notebook illustrates how `mHVSR-Vs30` estimates Vs30 from mHVSR measurements.\n",
    "\n",
    "The processing has been done following the SESAME (2004) guidelines.\n",
    "If you use this notebook, please also cite SESAME (2004) to recognize their original work.\n",
    "\n",
    "> SESAME (2004). Guidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations\n",
    "> Measurements, Processing, and Interpretation. European Commission - Research General Directorate, 62,\n",
    "> European Commission - Research General Directorate.\n",
    "\n",
    "To use this notebook, you need at least 30 minutes of ambient noise, sampled at 100 Hz, as measured on a three-component broadband sensor.\n",
    "\n",
    "This notebook predicts Vs30 with two separate models: Single-Mode ANN and Dual-Mode ANN. For the Single-Mode ANN, only the microtremor recording mentioned above is required. However, for the Dual-Mode ANN, you will also need the topographic features: station elevation and the average elevation around the station across a 1500 m diameter circle centered on the station. For the topographic features use the 1 arc-second Digital Elevation Model (DEM) for consistency with the model's development. If you do not have topographic features for your data, you can still just use the Single-Mode ANN by supplying \"NA\" for the topographic information.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. Follow the instructions in the software's [README.md](https://github.com/geoimaging/mhvsr-vs30?tab=readme-ov-file#getting-started) to get started by downloading the software and installing the dependencies.\n",
    "2. To run the default example, open this notebook in JupyterLab and select `Kernel > Restart Kernel and Run All Cells`.\n",
    "3. To try the other examples, uncomment the corresponding example number in the cell labeled __Data Input and Recording Check__ below before selecting `Kernel > Restart Kernel and Run All Cells`.\n",
    "4. Once you are comfortable running the examples provided, try supplying your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ad2cba5-6d18-4a3e-82ab-8ee2824ff419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747338898.934921   31089 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747338898.943484   31089 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747338898.964160   31089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747338898.964211   31089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747338898.964213   31089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747338898.964215   31089 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvsrpy\n",
    "from hvsrpy import sesame\n",
    "import obspy\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import joblib\n",
    "\n",
    "plt.style.use(hvsrpy.postprocessing.HVSRPY_MPL_STYLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96887ae-0243-4bdf-a7ce-7cdc94b84145",
   "metadata": {},
   "source": [
    "### Data Input and Recording Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92abd95a-02cc-48f9-ab01-a8ecc3199d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed duration is 60.00 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Data Input\n",
    "\n",
    "# example 1\n",
    "fname = \"./data/AR.STN01.A1.2019.006.mseed\"\n",
    "elevation_in_m =  2246 # Enter \"NA\" if unknown.\n",
    "elevation_1500m_avg_in_m =  2237.903656 # Enter \"NA\" if unknown.\n",
    "\n",
    "# example 2\n",
    "# fname = \"./data/CI.FUR-Rec288-Sen450.mseed\"\n",
    "# elevation_in_m =  -44 # Enter \"NA\" if unknown.\n",
    "# elevation_1500m_avg_in_m =  -43.80603662 # Enter \"NA\" if unknown.\n",
    "\n",
    "# # example 3\n",
    "# fname = \"./data/UT.STN09_20130320_020000.mseed\"\n",
    "# elevation_in_m =  35 # Enter \"NA\" if unknown.\n",
    "# elevation_1500m_avg_in_m =  35.33800078 # Enter \"NA\" if unknown.\n",
    "\n",
    "stream = obspy.read(fname)\n",
    "\n",
    "# Check trace count\n",
    "if len(stream) != 3:\n",
    "    print(\"Recording should have exactly three traces.\")\n",
    "else:\n",
    "    # Check sampling rates\n",
    "    sampling_rates = [tr.stats.sampling_rate for tr in stream]\n",
    "    if any(sr < 100 for sr in sampling_rates):\n",
    "        print(\"At least 100 Hz sampling rate is required. Found rates:\", sampling_rates)\n",
    "    else:\n",
    "        # Find latest start time and earliest end time\n",
    "        start_times = [tr.stats.starttime for tr in stream]\n",
    "        end_times = [tr.stats.endtime for tr in stream]\n",
    "\n",
    "        common_start = max(start_times)\n",
    "        common_end = min(end_times)\n",
    "\n",
    "        # Check duration\n",
    "        common_duration = common_end - common_start\n",
    "        if common_duration <= 0:\n",
    "            print(\"No overlapping time window between traces.\")\n",
    "        else:\n",
    "            # Trim all traces to common time window\n",
    "            stream.trim(starttime=common_start, endtime=common_end)\n",
    "\n",
    "            # Save to MiniSEED file\n",
    "            fpath = pathlib.Path(fname)\n",
    "            fname_updated = f\"{fname[:len(fpath.suffix)]}_updated{fpath.suffix}\"\n",
    "            stream.write(fname_updated, format=\"MSEED\")\n",
    "\n",
    "            # Check if duration exceeds 30 minutes\n",
    "            if common_duration >= 1800:  # 1800 seconds = 30 minutes\n",
    "                print(f\"Trimmed duration is {common_duration/60:.2f} minutes.\")\n",
    "            else:\n",
    "                print(\"Trimmed duration is less than 30 minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198aa980-0eca-4344-ad09-abf5a552b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Summary\n",
      "------------------------------------------------------------\n",
      "hvsrpy_version                           : 2.0.0\n",
      "orient_to_degrees_from_north             : 0.0\n",
      "filter_corner_frequencies_in_hz          : [None, None]\n",
      "window_length_in_seconds                 : 102.85714285714286\n",
      "detrend                                  : linear\n",
      "ignore_dissimilar_time_step_warning      : False\n",
      "preprocessing_method                     : hvsr\n",
      "Processing Summary\n",
      "------------------------------------------------------------\n",
      "hvsrpy_version                           : 2.0.0\n",
      "window_type_and_width                    : ('tukey', 0.2)\n",
      "smoothing                                :\n",
      "     operator                            : konno_and_ohmachi\n",
      "     bandwidth                           : 40\n",
      "     center_frequencies_in_hz            : [0.14776046176014435 ... 6371930790751, 50.0]\n",
      "fft_settings                             : None\n",
      "handle_dissimilar_time_steps_by          : frequency_domain_resampling\n",
      "processing_method                        : traditional\n",
      "method_to_combine_horizontals            : geometric_mean\n"
     ]
    }
   ],
   "source": [
    "# HVSR Preprocessing Settings\n",
    "preprocessing_settings = hvsrpy.settings.HvsrPreProcessingSettings()\n",
    "preprocessing_settings.detrend = \"linear\"\n",
    "significant_cycles = 15  # require 15 significant cycles.\n",
    "time_windows = 35  # require 35 time windows.\n",
    "duration_in_seconds = common_duration  # window length (s)\n",
    "windowlength_in_seconds = duration_in_seconds / time_windows\n",
    "preprocessing_settings.window_length_in_seconds = windowlength_in_seconds\n",
    "\n",
    "print(\"Preprocessing Summary\")\n",
    "print(\"-\"*60)\n",
    "preprocessing_settings.psummary()\n",
    "\n",
    "# HVSR Processing Settings\n",
    "processing_settings = hvsrpy.settings.HvsrTraditionalProcessingSettings()\n",
    "processing_settings.window_type_and_width = (\"tukey\", 0.2)\n",
    "processing_settings.smoothing=dict(operator=\"konno_and_ohmachi\",\n",
    "                                   bandwidth=40,\n",
    "                                   center_frequencies_in_hz=np.geomspace(0.05, 50, 256))\n",
    "processing_settings.method_to_combine_horizontals = \"geometric_mean\"\n",
    "processing_settings.handle_dissimilar_time_steps_by = \"frequency_domain_resampling\"\n",
    "\n",
    "desired_frequency_vector_in_hz = np.geomspace(0.05, 50, 256)\n",
    "minimum_frequency = significant_cycles / windowlength_in_seconds\n",
    "fids = desired_frequency_vector_in_hz > minimum_frequency\n",
    "frequency_resampling_in_hz = desired_frequency_vector_in_hz[fids]\n",
    "processing_settings.smoothing[\"center_frequencies_in_hz\"] = frequency_resampling_in_hz\n",
    "\n",
    "print(\"Processing Summary\")\n",
    "print(\"-\"*60)\n",
    "processing_settings.psummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f38bf2-82f2-465d-b57f-6c07b192c31f",
   "metadata": {},
   "source": [
    "### HVSR Processing and Manual Window Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c6bd22-127f-4bf8-900c-7861ba547a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute HVSR\n",
    "srecords = hvsrpy.read([fname_updated])\n",
    "srecords = hvsrpy.preprocess(srecords, preprocessing_settings)\n",
    "hvsr = hvsrpy.process(srecords, processing_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d2a816-2082-40a0-b600-cc8bf27df9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HvsrTraditional object\n",
    "mhvsr = hvsrpy.HvsrTraditional(frequency=hvsr.frequency, amplitude=hvsr.amplitude)\n",
    "\n",
    "# Perform manual window rejection\n",
    "hvsrpy.window_rejection.manual_window_rejection(\n",
    "    mhvsr, y_limit=15, plot_frequency_std=False, fig=None, ax=None  #Change y_limit as required in the plot.\n",
    ")\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6d274-f36e-4004-89b5-5fbf7382853f",
   "metadata": {},
   "source": [
    "### Parameters for high dim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cac0256-9667-4028-af28-53ac6d0ee7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HVSR Features\n",
    "# Original frequency and amplitude arrays\n",
    "freqs = mhvsr.frequency  # original frequency array\n",
    "amps = mhvsr.mean_curve(distribution=\"lognormal\")  #Taking the lognormal mean curve among the accepted windows.\n",
    "\n",
    "# Step 1: Trim to 0.3–50 Hz range\n",
    "mask = (freqs >= 0.3) & (freqs <= 50)\n",
    "freqs_trimmed = freqs[mask]\n",
    "amps_trimmed = amps[mask]\n",
    "\n",
    "# Step 2: Create 35 log-spaced frequencies between 0.3 and 50 Hz\n",
    "resampled_freqs = np.logspace(np.log10(0.3), np.log10(50), 35)\n",
    "\n",
    "# Step 3: Interpolate amplitudes linearly\n",
    "interp_func = interp1d(freqs_trimmed, amps_trimmed, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "resampled_amps = interp_func(resampled_freqs)\n",
    "\n",
    "X_hvsr = pd.DataFrame([resampled_amps], columns=[f for f in resampled_freqs])\n",
    "\n",
    "if elevation_in_m!=\"NA\" and elevation_1500m_avg_in_m!=\"NA\":\n",
    "    dual_ANN = True\n",
    "    # Topographic Features\n",
    "    TPI = elevation_in_m - elevation_1500m_avg_in_m\n",
    "    \n",
    "    # Assemble into DataFrame\n",
    "    X_topo = pd.DataFrame([{\n",
    "        \"TPI\": TPI,\n",
    "        \"elevation\": elevation_in_m\n",
    "    }])\n",
    "    \n",
    "    # Elevation Binning\n",
    "    elevation_bins = [-500, 0, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "    elevation_labels = [\n",
    "        'Elevation_Bin_[-500.0, 0.0)',\n",
    "        'Elevation_Bin_[0.0, 500.0)',\n",
    "        'Elevation_Bin_[500.0, 1000.0)',\n",
    "        'Elevation_Bin_[1000.0, 1500.0)',\n",
    "        'Elevation_Bin_[1500.0, 2000.0)',\n",
    "        'Elevation_Bin_[2000.0, 2500.0)',\n",
    "        'Elevation_Bin_[2500.0, 3000.0)'\n",
    "    ]\n",
    "    \n",
    "    # Bin the elevation into categories\n",
    "    X_topo = X_topo.copy()\n",
    "    X_topo['Elevation_Bin'] = pd.cut(X_topo['elevation'], bins=elevation_bins, labels=elevation_labels, right=False)\n",
    "    \n",
    "    # One-hot encode the elevation bin\n",
    "    elevation_dummies = pd.get_dummies(X_topo['Elevation_Bin']).astype(int)\n",
    "    \n",
    "    # Concatenate back to original DataFrame\n",
    "    X_topo = pd.concat([X_topo, elevation_dummies], axis=1)\n",
    "    \n",
    "    # Optional: Drop 'Elevation_Bin' column if you only need one-hot encoded version\n",
    "    X_topo = X_topo.drop(columns=['elevation','Elevation_Bin'])\n",
    "else:\n",
    "    dual_ANN = False\n",
    "    print(\"Only Single mode ANN model can be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73cbbe86-e334-4d8f-b6ba-5a7df4b314b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.300000</th>\n",
       "      <th>0.348714</th>\n",
       "      <th>0.405339</th>\n",
       "      <th>0.471158</th>\n",
       "      <th>0.547665</th>\n",
       "      <th>0.636596</th>\n",
       "      <th>0.739967</th>\n",
       "      <th>0.860123</th>\n",
       "      <th>0.999791</th>\n",
       "      <th>1.162138</th>\n",
       "      <th>...</th>\n",
       "      <th>12.907246</th>\n",
       "      <th>15.003137</th>\n",
       "      <th>17.439361</th>\n",
       "      <th>20.271181</th>\n",
       "      <th>23.562835</th>\n",
       "      <th>27.388991</th>\n",
       "      <th>31.836442</th>\n",
       "      <th>37.006075</th>\n",
       "      <th>43.015157</th>\n",
       "      <th>50.000000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.70023</td>\n",
       "      <td>2.746539</td>\n",
       "      <td>6.006496</td>\n",
       "      <td>10.087151</td>\n",
       "      <td>7.861836</td>\n",
       "      <td>5.539016</td>\n",
       "      <td>3.996405</td>\n",
       "      <td>2.770711</td>\n",
       "      <td>1.665763</td>\n",
       "      <td>0.798002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420818</td>\n",
       "      <td>0.531571</td>\n",
       "      <td>0.566392</td>\n",
       "      <td>0.641381</td>\n",
       "      <td>0.707136</td>\n",
       "      <td>0.801803</td>\n",
       "      <td>0.863421</td>\n",
       "      <td>0.983277</td>\n",
       "      <td>1.234071</td>\n",
       "      <td>1.266722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.300000   0.348714   0.405339   0.471158   0.547665   0.636596   \\\n",
       "0    1.70023   2.746539   6.006496  10.087151   7.861836   5.539016   \n",
       "\n",
       "   0.739967   0.860123   0.999791   1.162138   ...  12.907246  15.003137  \\\n",
       "0   3.996405   2.770711   1.665763   0.798002  ...   0.420818   0.531571   \n",
       "\n",
       "   17.439361  20.271181  23.562835  27.388991  31.836442  37.006075  \\\n",
       "0   0.566392   0.641381   0.707136   0.801803   0.863421   0.983277   \n",
       "\n",
       "   43.015157  50.000000  \n",
       "0   1.234071   1.266722  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_hvsr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427265a-a8c3-493c-8336-38fa0c8ae877",
   "metadata": {},
   "source": [
    "### Load high-dimensional models and Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69292a4-f4f3-410d-a6ae-8aebd1223325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1747338922.345180   31089 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1747338922.346729   31089 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Load Models\n",
    "single_mode_ann_model = tf.keras.models.load_model(\"./models/log_ANN_model.keras\")\n",
    "X_hvsr_scaled = np.log(X_hvsr.to_numpy().astype(np.float32)) \n",
    "\n",
    "if dual_ANN:\n",
    "    dual_mode_ann_model = tf.keras.models.load_model(\"./models/Multi-input_log_ANN.keras\")\n",
    "    topo_scaler = joblib.load(\"./models/log_model_standard_scaler_metadata.pkl\")\n",
    "    X_topo_scaled = topo_scaler.transform(X_topo.to_numpy().astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf8244-2133-4e86-8e6c-90eb704c72b4",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7772a59a-80cd-48d7-bf64-56f0387b7525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vs30 (m/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Single Mode ANN</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dual mode ANN</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Vs30 (m/s)\n",
       "0  Single Mode ANN          75\n",
       "1    Dual mode ANN          57"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always predict with single mode\n",
    "Vs30_pred_single_ann = single_mode_ann_model.predict(X_hvsr_scaled).flatten()\n",
    "\n",
    "models = [\"Single Mode ANN\"]\n",
    "predictions = [np.exp(Vs30_pred_single_ann)]\n",
    "\n",
    "# If topo features are available.\n",
    "if dual_ANN:\n",
    "    Vs30_pred_dual_ann = dual_mode_ann_model.predict([X_hvsr_scaled, X_topo_scaled]).flatten()\n",
    "    models.append(\"Dual mode ANN\")\n",
    "    predictions.append(np.exp(Vs30_pred_dual_ann))\n",
    "\n",
    "# Create DataFrame\n",
    "vs30_modelwise_df = pd.DataFrame({\n",
    "    \"Model\": np.repeat(models, [len(p) for p in predictions]),\n",
    "    \"Vs30 (m/s)\": np.round(np.concatenate(predictions), 0).astype(int)\n",
    "})\n",
    "\n",
    "# Show the result\n",
    "vs30_modelwise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5024194-bfed-4830-812e-054fd201c417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
